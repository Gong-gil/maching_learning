{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07cf671f",
   "metadata": {},
   "source": [
    "# 크롤링 핵심코드 패턴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a697379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "잔금대출에도 DTI 규제 적용 검토\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "res = requests.get('http://v.media.daum.net/v/20170615203441266')\n",
    "soup = BeautifulSoup(res.content, 'html.parser')\n",
    "mydata = soup.find('title')\n",
    "print(mydata.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49cad4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "웹페이지에서 필요한 데이터를 추출하는 것 \n",
      "\n",
      "[1]크롤링이란? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "html =      \"<html> \\\n",
    "                <body> \\\n",
    "                        <h1 id='title'>[1]크롤링이란?</h1> \\\n",
    "                        <p class='cssstyle'>웹페이지에서 필요한 데이터를 추출하는 것</p> \\\n",
    "                        <p id='body' align='center'>파이썬을 중심으로 다양한 웹크롤링 기술 발달</p> \\\n",
    "                </body> \\\n",
    "            </html>\"\n",
    "soup = BeautifulSoup(html,\"html.parser\")\n",
    "# 태그로 검색 방법\n",
    "data = soup.find('p')    # 맨 처음 나온 p가 나온다.\n",
    "print(data.string,\"\\n\")\n",
    "data = soup.find('h1')    # 맨 처음 나온 h1이 나온다.\n",
    "print(data.string,\"\\n\")   # get_text 기법도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21f856d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = soup.find('p', class_ = 'cssstyle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6fe6f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "웹페이지에서 필요한 데이터를 추출하는 것 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data.string,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33967e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "웹페이지에서 필요한 데이터를 추출하는 것 \n",
      "\n",
      "파이썬을 중심으로 다양한 웹크롤링 기술 발달 \n",
      "\n",
      "파이썬을 중심으로 다양한 웹크롤링 기술 발달 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_1 = soup.find('p', 'cssstyle')\n",
    "data_2 = soup.find('p', attrs = {'align' : 'center'})\n",
    "data_3 = soup.find(id = 'body')\n",
    "\n",
    "print(data_1.string,\"\\n\")\n",
    "print(data_2.string,\"\\n\")\n",
    "print(data_3.string,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc724a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "웹페이지에서 필요한 데이터를 추출하는 것\n",
      "파이썬을 중심으로 다양한 웹크롤링 기술 발달\n"
     ]
    }
   ],
   "source": [
    "# 해당하는 모든 태그를 불러온다.\n",
    "\n",
    "paragraph_data = soup.find_all('p')   # p태그가 들어간 모든 문장을 가져온다.\n",
    "\n",
    "for paragraph in paragraph_data:\n",
    "    print(paragraph.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c063ee3c",
   "metadata": {},
   "source": [
    "## Html / CSS 언어 이해를 기반으로 크롤링해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "22fa21c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "잔금대출에도 DTI 규제 적용 검토\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "res = requests.get('http://v.media.daum.net/v/20170615203441266')\n",
    "soup = BeautifulSoup(res.content, 'html.parser')\n",
    "title = soup.find('h3', 'tit_view')\n",
    "print(title.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0c4cb490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(왕초보) - 클래스 소개\n",
      "(왕초보) - 블로그 개발 필요한 준비물 준비하기\n",
      "(왕초보) - Github pages 설정해서 블로그 첫 페이지 만들어보기\n",
      "(왕초보) - 초간단 페이지 만들어보기\n",
      "(왕초보) - 이쁘게 테마 적용해보기\n",
      "(왕초보) - 마크다운 기초 이해하고, 실제 나만의 블로그 페이지 만들기\n",
      "(왕초보) - 다양한 마크다운 기법 익혀보며, 나만의 블로그 페이지 꾸며보기\n",
      "(초급) - 강사가 실제 사용하는 자동 프로그램 소개 [2]\n",
      "(초급) - 필요한 프로그램 설치 시연 [5]\n",
      "(초급) - 데이터를 엑셀 파일로 만들기 [9]\n",
      "(초급) -     엑셀 파일 이쁘게! 이쁘게! [8]\n",
      "(초급) -     나대신 주기적으로 파이썬 프로그램 실행하기 [7]\n",
      "(초급) - 파이썬으로 슬랙(slack) 메신저에 글쓰기 [40]\n",
      "(초급) - 웹사이트 변경사항 주기적으로 체크해서, 메신저로 알람주기 [12]\n",
      "(초급) - 네이버 API 사용해서, 블로그에 글쓰기 [42]\n",
      "(중급) - 자동으로 쿠팡파트너스 API 로 가져온 상품 정보, 네이버 블로그/트위터에 홍보하기 [412]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "res = requests.get('https://davelee-fun.github.io/blog/crawl_test')\n",
    "soup = BeautifulSoup(res.content, 'html.parser')\n",
    "titles = soup.find_all('li','course')\n",
    "for title in titles:\n",
    "    print(title.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd7dd29",
   "metadata": {},
   "source": [
    "### 추출한 것에서 또 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d7c9fc5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(왕초보) - 클래스 소개\n",
      "(왕초보) - 블로그 개발 필요한 준비물 준비하기\n",
      "(왕초보) - Github pages 설정해서 블로그 첫 페이지 만들어보기\n",
      "(왕초보) - 초간단 페이지 만들어보기\n",
      "(왕초보) - 이쁘게 테마 적용해보기\n",
      "(왕초보) - 마크다운 기초 이해하고, 실제 나만의 블로그 페이지 만들기\n",
      "(왕초보) - 다양한 마크다운 기법 익혀보며, 나만의 블로그 페이지 꾸며보기\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "res = requests.get('https://davelee-fun.github.io/blog/crawl_test')\n",
    "soup = BeautifulSoup(res.content, 'html.parser')\n",
    "\n",
    "section = soup.find('ul', id = 'hobby_course_list')\n",
    "titles = section.find_all('li','course')\n",
    "for title in titles:\n",
    "    print(title.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f5535788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(초급) - 강사가 실제 사용하는 자동 프로그램 소개 [2]\n",
      "(초급) - 필요한 프로그램 설치 시연 [5]\n",
      "(초급) - 데이터를 엑셀 파일로 만들기 [9]\n",
      "(초급) -     엑셀 파일 이쁘게! 이쁘게! [8]\n",
      "(초급) -     나대신 주기적으로 파이썬 프로그램 실행하기 [7]\n",
      "(초급) - 파이썬으로 슬랙(slack) 메신저에 글쓰기 [40]\n",
      "(초급) - 웹사이트 변경사항 주기적으로 체크해서, 메신저로 알람주기 [12]\n",
      "(초급) - 네이버 API 사용해서, 블로그에 글쓰기 [42]\n",
      "(중급) - 자동으로 쿠팡파트너스 API 로 가져온 상품 정보, 네이버 블로그/트위터에 홍보하기 [412]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "res = requests.get('https://davelee-fun.github.io/blog/crawl_test')\n",
    "soup = BeautifulSoup(res.content, 'html.parser')\n",
    "\n",
    "section = soup.find('ul', id = 'dev_course_list')\n",
    "titles = section.find_all('li','course')\n",
    "for title in titles:\n",
    "    print(title.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e49163c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 강사가 실제 사용하는 자동 프로그램 소개\n",
      "2. 필요한 프로그램 설치 시연\n",
      "3. 데이터를 엑셀 파일로 만들기\n",
      "4. 엑셀 파일 이쁘게! 이쁘게!\n",
      "5. 나대신 주기적으로 파이썬 프로그램 실행하기\n",
      "6. 파이썬으로 슬랙(slack) 메신저에 글쓰기\n",
      "7. 웹사이트 변경사항 주기적으로 체크해서, 메신저로 알람주기\n",
      "8. 네이버 API 사용해서, 블로그에 글쓰기\n",
      "9. 자동으로 쿠팡파트너스 API 로 가져온 상품 정보, 네이버 블로그/트위터에 홍보하기\n"
     ]
    }
   ],
   "source": [
    "# 위에 정보를 변형해서 가지고왔당\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "res = requests.get('https://davelee-fun.github.io/blog/crawl_test')\n",
    "soup = BeautifulSoup(res.content, 'html.parser')\n",
    "section = soup.find('ul',  id = 'dev_course_list')\n",
    "titles = section.find_all('li','course')\n",
    "\n",
    "for index, title in enumerate(titles):\n",
    "    print(str(index + 1) + '.', title.get_text().split('-')[1].split('[')[0].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0daadb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
