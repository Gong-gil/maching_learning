{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d307c3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3014f524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 1 \n",
      "\n",
      "['The Matrix is everywher its all aroun us, here even in this room.You can see it out your window or on your television.You feel it when you ho to work, or go to church or pay your taxes.']\n"
     ]
    }
   ],
   "source": [
    "# 문장 토큰화 : sent_tokenize\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "text_sample = 'The Matrix is everywher its all aroun us, here even in this room.\\\n",
    "You can see it out your window or on your television.\\\n",
    "You feel it when you ho to work, or go to church or pay your taxes.'\n",
    "\n",
    "sentences = sent_tokenize(text=text_sample)\n",
    "\n",
    "print(type(sentences), len(sentences), '\\n')\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbc1c5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 14 \n",
      "\n",
      "['The', 'Natrix', 'is', 'everywhere', 'its', 'all', 'aroun', 'us', ',', 'here', 'even', 'in', 'this', 'room']\n"
     ]
    }
   ],
   "source": [
    "# 단어 토큰화 : word_tokenize\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "sentence = 'The Natrix is everywhere its all aroun us, here even in this room'\n",
    "\n",
    "words = word_tokenize(sentence)\n",
    "print(type(words), len(words), '\\n')\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a5b4e62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 14 \n",
      "\n",
      "['The', 'Natrix', 'is', 'everywhere', 'its', 'all', 'aroun', 'us', ',', 'here', 'even', 'in', 'this', 'room']\n"
     ]
    }
   ],
   "source": [
    "# 여러 문장들에 대한 단어 토큰화\n",
    "\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "# 여러개의 문장으로 된 입력 데이터를 문장별로 단어 토큰화 만드는 함수 생성\n",
    "def tokenize_text(text):\n",
    "    \n",
    "    #문장별로 분리 토큰\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    #분리된 문장별 단어 토큰화\n",
    "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens\n",
    "\n",
    "# 여러 문장들에 대해 문장별 단어 토큰화 수행\n",
    "word_tokens = tokenize_text(text_sample)\n",
    "print(type(words), len(words), '\\n')\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a84831b",
   "metadata": {},
   "source": [
    "## stopwords 제거하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72362f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e6bf218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 stopwords 갯수:  179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
     ]
    }
   ],
   "source": [
    "# 영어 stopwords 확인\n",
    "print('영어 stopwords 갯수: ', len(nltk.corpus.stopwords.words('english')))\n",
    "\n",
    "# 영어 stopwords 중 10개만 출력해보자\n",
    "print(nltk.corpus.stopwords.words('english')[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d0be2b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['matrix', 'everywher', 'aroun', 'us', ',', 'even', 'room.you', 'see', 'window', 'television.you', 'feel', 'ho', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "#위 예제 3개 문장에서 얻은 단어 토큰에 대해 sropwords 제거해보자\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "all_tokens = []\n",
    "\n",
    "# 위 예제의 3개의 문장별로 얻은 word_tokens list에 대해 stop word 제거 Loop\n",
    "for sentence in word_tokens:       \n",
    "    filtered_words = []\n",
    "    # 개별 문장별로 tokenize 된 sentence list에 대해 stopword 제거 loop\n",
    "    for word in sentence:\n",
    "        #소문자로 모두 변환합니다\n",
    "        word = word.lower()\n",
    "        # tokenize 된 개별 word가 stopwords들 단어에 포함되지 않으면 word_tokwns에 추가\n",
    "        if word not in stopwords:\n",
    "            filtered_words.append(word)\n",
    "    all_tokens.append(filtered_words)\n",
    "\n",
    "print(all_tokens)\n",
    "\n",
    "# the,is등 stopwords가 제거된 것 확인 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15503772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n",
      "amus amus amus\n",
      "happy happiest\n",
      "fant fanciest\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "print(stemmer.stem('working'),stemmer.stem('works'),stemmer.stem('worked'))\n",
    "print(stemmer.stem('amusing'),stemmer.stem('amuses'),stemmer.stem('amused'))\n",
    "print(stemmer.stem('happier'),stemmer.stem('happiest'))\n",
    "print(stemmer.stem('fancier'),stemmer.stem('fanciest'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b04779c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "09ad163d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amuse amuse amuse\n",
      "happy happiesr\n",
      "fancy fancy\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "print(lemma.lemmatize('amusing','v'),lemma.lemmatize('amuses','v'),lemma.lemmatize('amused','v'))\n",
    "print(lemma.lemmatize('happier','a'),lemma.lemmatize('happiesr','a'))\n",
    "print(lemma.lemmatize('fancier','a'),lemma.lemmatize('fanciest','a'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53833b6",
   "metadata": {},
   "source": [
    "# COO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "90f0df8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dense = np.array([[3,0,1],[0,2,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9398eb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "data = np.array([3,1,2])\n",
    "\n",
    "row_pos = np.array([0,0,1])\n",
    "col_pos = np.array([0,2,1])\n",
    "\n",
    "sparse_coo =sparse.coo_matrix((data, (row_pos,col_pos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7371aad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 0, 1],\n",
       "       [0, 2, 0]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_coo.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcbb231",
   "metadata": {},
   "source": [
    "# CSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54629e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "dense2 = np.array([[0,0,1,0,0,5],\n",
    "                   [1,4,0,3,2,5],\n",
    "                   [0,6,0,3,0,0],\n",
    "                   [2,0,0,0,0,0],\n",
    "                   [0,0,0,7,0,8],\n",
    "                   [1,0,0,0,0,0]])\n",
    "\n",
    "data2 = np.array([1,5,1,4,3,2,5,6,3,2,7,8,1])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
